%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Define Article %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Using Packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{empheq}
\usepackage{mdframed}
\usepackage{booktabs}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{color}
\usepackage{psfrag}
\usepackage{pgfplots}
\usepackage{bm}
\usepackage{listings}
\usepackage{tabularx}
\usepackage{array}
\usepackage{subcaption}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Other Settings

%%%%%%%%%%%%%%%%%%%%%%%%%% Page Setting %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\geometry{a4paper}

%%%%%%%%%%%%%%%%%%%%%%%%%% Define some useful colors %%%%%%%%%%%%%%%%%%%%%%%%%%
\definecolor{ocre}{RGB}{243,102,25}
\definecolor{mygray}{RGB}{243,243,244}
\definecolor{deepGreen}{RGB}{26,111,0}
\definecolor{shallowGreen}{RGB}{235,255,255}
\definecolor{deepBlue}{RGB}{61,124,222}
\definecolor{shallowBlue}{RGB}{235,249,255}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Listing Settings %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%% Define an orangebox command %%%%%%%%%%%%%%%%%%%%%%%%
\newcommand\orangebox[1]{\fcolorbox{ocre}{mygray}{\hspace{1em}#1\hspace{1em}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%% English Environments %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheoremstyle{mytheoremstyle}{3pt}{3pt}{\normalfont}{0cm}{\rmfamily\bfseries}{}{1em}{{\color{black}\thmname{#1}~\thmnumber{#2}}\thmnote{\,--\,#3}}
\newtheoremstyle{myproblemstyle}{3pt}{3pt}{\normalfont}{0cm}{\rmfamily\bfseries}{}{1em}{{\color{black}\thmname{#1}~\thmnumber{#2}}\thmnote{\,--\,#3}}
\theoremstyle{mytheoremstyle}
\newmdtheoremenv[linewidth=1pt,backgroundcolor=shallowGreen,linecolor=deepGreen,leftmargin=0pt,innerleftmargin=20pt,innerrightmargin=20pt,]{theorem}{Theorem}[section]
\theoremstyle{mytheoremstyle}
\newmdtheoremenv[linewidth=1pt,backgroundcolor=shallowBlue,linecolor=deepBlue,leftmargin=0pt,innerleftmargin=20pt,innerrightmargin=20pt,]{definition}{Definition}[section]
\theoremstyle{myproblemstyle}
\newmdtheoremenv[linecolor=black,leftmargin=0pt,innerleftmargin=10pt,innerrightmargin=10pt,]{problem}{Problem}[section]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Plotting Settings %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepgfplotslibrary{colorbrewer}
\pgfplotsset{width=8cm,compat=1.9}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Title & Author %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Freedom of Press Among World Nations}
\author{Shakleen Ishfar \and Eugene Ayonga}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
    \maketitle

    \section{Introduction}

    There are many news organizations around the world. News organizations play a vital role in relaying important news of home and abroad to its readers. These news often cover incidents that are either positive, negative, or neutral. Moreover, some news stories can be viewed as talking for a country or talking against it. 

    Freedom of speech is the right to express ideas and opinions without censorship, restraint, or fear of retribution. A news outlet is free if it can report news in an unbiased manner and free from censorship. In this project, we aim to detect if a local news organization is free. To do so, we compare the sentiment and stance of the organization with international news reporting institutions \textbf{Reuters} and \textbf{Associated Press}. We assume that the more similar a local news is, in terms of sentiment and stance, to the international media outlets, the more free it is. 
    
    In summary, this study investigates freedom of speech in local news across countries, examining topic-specific distinctions by comparing sentiment and stance scores with international sources to reveal distinctions and assess agreement levels.

    \section{Related Work}
    % TODO: Cite similar works

    \section{Methodology}

    The methodlogy of our study can be summarized as follows:

    \begin{enumerate}
        \item \textbf{Data Accumulation}: Select countries to study and collect local-international news articles specific to these countries.
        \item \textbf{Data Processing}: We prepared the data for analysis by performing text processing.
        \item \textbf{Topic Modeling}: To compare relevant news articles from both sources we need to first find the topics and categorize news into these topics.
        \item \textbf{Sentiment and stance analysis}: For each news article, we need to find its sentiment and stance. Specifically, by sentiment, we mean whether a news is positive or neutral or negative. And by stance, we mean whether the news is in support of the local nation or impartial or against it.
        \item \textbf{Hypothesis testing}: Finally, we perform hypothesis testing on the sentiment and stance score for both source of news. This allows us to come to statistically significant conclusions.
    \end{enumerate}

    \subsection{Data Acquisition}

    For this study, we decided to move forward with three countries, Canada, China, and Russia. The countries were picked based on political inclinations. For each of these countries, we collected news from local and international sources. Table \ref{table:data_sources} presents our data sources for each countries.

    \begin{table}[ph]
        \centering
        \begin{tabular}{|c|l|l|l|}
            \hline
            Source Type & Canada & China & Russia \\
            \hline
            Local & Global News & China Daily & The Moscow Times \\
            \hline
            International & \multicolumn{3}{c|}{Reuters and Associated Press} \\
            \hline
        \end{tabular}
        \caption{Data sources for countries under study}
        \label{table:data_sources}
    \end{table}

    We used \emph{Selenium} and \emph{News Please} to collect our news corpus. In particular, we crawled news websites using Selenium and collected articles URLs. Afterwards, we scraped article data using \emph{News Please}. News Please gives us a lot of data about the articles. \cite{Hamborg2017} The following are properties of interest for our study:

    \begin{enumerate}
        \item \textbf{Title}: The title of the articles.
        \item \textbf{Description}: A short description of the article. Typically, the subheading of the article.
        \item \textbf{Maintext}: The body of the article.
        \item \textbf{Publication Date}: When the article was published.
    \end{enumerate}

    Our accumulated data and their countrywise distribution is shown in figure \ref{fig:raw_data}.
    
    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.75\linewidth]{../images/plots/barchart_scraped_data.png}
        \caption{Collected data across countries of interest from local and international sources}
        \label{fig:raw_data}
    \end{figure}

    \subsection{Data Processing}

    To prepare the data for topic modeling and sentiment-stance analysis we had to process the dataset. This inlcludes the following:

    \begin{enumerate}
        \item \textbf{Imputation}: Many of the URLs scraped by News Please, some attributes were null. The first step was filling in those values. We chose to replace null text columns with empty strings.
        \item \textbf{Duplicate removal}: In this step, we detected duplicate article data and removed them. In particular, we detected duplicates by comparies the \emph{publication data} and \emph{title} attributes.
        \item \textbf{Text level processing} \begin{itemize}
            \item Remove HTML tags, links, emails, phone numbers, etc.
            \item Discard editorial information, e.g. "Published by Global News".
            \item Strip non-ASCII characters.
            \item Remove editorial information, e.g. written by X, photo by Y, etc.
        \end{itemize}
        \item \textbf{Row level processing} \begin{itemize}
            \item Drop rows with empty titles and maintext.
            \item Filter out unrelated geographical news.
            \item Remove rows where the maintext is too short.
        \end{itemize}
        \item \textbf{Country level processing} \begin{itemize}
            \item Global news in particular reports inflation news. This sort of articles don't have a lot of text and is strictly tabular. We decided not to include this sort of news in our analysis.
            \item Removed Israel-Palestine from all country news corpus. As they are not relevant to the countries being studied.
        \end{itemize}
    \end{enumerate}

    \subsection{Topic Modeling}

    BERTopic is an unsupervised machine Learning  algorithm for topic modeling. It leverages Bidirectional Encoder Representations from Transformers (BERT) and c-TF-IDF (Class-Based Term Frequency - Inverse Document Frequency) to create coherent and easily interpretable topics, described by automatically generated labels. It has a number of sub-components, which are discussed below, whose understanding at a higher level can inherently improve the topic modeling performance. \cite{grootendorst2022bertopic}
    
    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.5\linewidth]{../images/BERTopic_diagram.png}
        \caption{BERTopic Diagram}
        \label{fig:bertopic_diagram}
    \end{figure}

    \subsubsection{Transformer Embedding Model}
    The model embeds Input documents to sentence vector representations which are typically dense in nature. In this project, we used sentence transformers to capture semantic similarity, as it provides an extensive library of high performing sentence embeddings. The L6-v2 version created 384-dimensional sentence embeddings.

    \subsubsection{Dimensionality Reduction}
    As a next step, BERTopic then performs dimensionality reduction of the 384-dimension sentence embeddings into a lower-dimensional space, either two or three-dimensional vectors. This project used the BERTopic default; Uniform Manifold Approximation and Projection (UMAP). UMAP captures both the local and global high-dimensional space in lower dimensions.

    Since BERTopic provides room for independence between steps, other popular choices for dimensionality reduction such as Principal Components Analysis (PCA) and t-distributed Stochastic Neighbour Embedding (t-SNE) can be used.

    PCA works by preserving larger distances by mean squared error. This results in the data's global structure being reserved; a pro when there are clusters that are easily distinguishable in the dataset. A demerit of using PCA is that it falls short when dealing with more nuanced data where local structures are vital. Inversely, t-SNE main characteristic is the preservation of similarity. UMAP exploits the best of both worlds, thus an appropriate technique.

    \subsubsection{Clustering}

    Involves clustering dimensionally reduced embeddings into groups of similar embeddings to extract topics. There are an array of clustering techniques such as partition-based, hierarchical based and density-based, to name but a few. Each one of them has its own advantages and disadvantages.

    Hierarchical Density Based Spatial Clustering of Applications with Noise (HDBSCAN) is both hierarchical and density-based. It is BERTopic's default that is typically used to capture structures with different densities. Advantages of HDBSCAN include: ease of tuning and visualizing hierarchical data, outlier identification and the ability to cluster irregular shapes. 

    Since BERTopic's independence between steps still hold, K-means can be used for clustering using a centroid approach. A drawback would be poor handling of noisy clusters.

    \subsubsection{Vectorizers}

    As a final step of the BERTopic workflow, topic extraction from the clusters formed in the previous step is done. BERTopic applies the use of a modified version of TF-IDF called Class-Based Term Frequency â€“ Inverse Document Frequency (c-TF-IDF), which finds the most essential and relevant terms given to each document within a cluster. A CountVectorizer is initialized, and together with TF-IDF, topic representations are created, allowing for parameter fine tuning.

    \begin{figure}[hp]

        \centering
        \begin{subfigure}{0.45\textwidth}
            \includegraphics[width=\textwidth]{../images/plots/Canada/intertopic_distance_map_canada.png}
            \caption{Canada Intertopic Map}
            \label{fig:intertopic_canada}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.45\textwidth}
            \includegraphics[width=\textwidth]{../images/plots/Canada/similarity_matrix_canada.png}
            \caption{Canada Similarity Matrix}
            \label{fig:sim_canada}
        \end{subfigure}
        \hfill
        
        \begin{subfigure}{0.45\textwidth}
            \includegraphics[width=\textwidth]{../images/plots/China/intertopic_distance_map_china.png}
            \caption{China Intertopic Map}
            \label{fig:intertopic_china}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.45\textwidth}
            \includegraphics[width=\textwidth]{../images/plots/China/similarity_matrix_china.png}
            \caption{China Similarity Matrix}
            \label{fig:sim_china}
        \end{subfigure}
        \hfill

        \begin{subfigure}{0.45\textwidth}
            \includegraphics[width=\textwidth]{../images/plots/Russia/intertopic_distance_map_russia.png}
            \caption{Russia Intertopic Map}
            \label{fig:intertopic_russia}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.45\textwidth}
            \includegraphics[width=\textwidth]{../images/plots/Russia/similarity_matrix_russia.png}
            \caption{Russia Similarity Matrix}
            \label{fig:sim_russia}
        \end{subfigure}
                
        \caption{Intertopic distance maps and similarity matrices for accessing quality of topic models}
        \label{fig:topic_model_quality}
    \end{figure}

    \subsubsection{Topic Quality}
    After tuning the different parameters, we were able to generate topics for our collected data. Figure \ref{fig:topic_model_quality} shows the quality of the topic models. A good topic model will have topics that have low intertopic similarity. Moreover, the bubbles in the intertopic distance map should be large and non-overlapping. The figures clearly show this for our modeled topics. Here are a few interesting observations:

    \begin{enumerate}
        \item Canadian news has the most amount of topics. Moreover, most of the topics seem to be quite dissimilar to one another.
        \item Chinese news topics have more similarity between topics. But the media covers a wide array of topics in general.
        \item Russian news media seem to cover more political news. This is expected as there is a war that's ongoing.
    \end{enumerate}

    \subsection{Sentiment and Stance Analysis}

    For sentiment and stance analysis, we decided to use a Large Language Model (LLM). 
    
    \subsubsection{Utilization of LLM}
    LLMs are excellent for natural language understanding and generation tasks. We used these capabilities of an LLM in the following way:

    \begin{enumerate}
        \item \textbf{Classification}: Classify the document's sentiment as positive or negative or neutral. Classify the stance as for or neutral or against the local country.
        \item \textbf{Scoring}: Assign a score from -1.0 (negative / against) to +1.0 (positive / for) for sentiment and stance.
        \item \textbf{Reason}: The LLM gives a short one sentence reason behind its classification and scoring.
    \end{enumerate}

    We decided to use the open-source \textbf{LLaMa-2} model, from \emph{Meta}. For our training and inference pipeline we heavily relied on \emph{HuggingFace}.

    \subsubsection{Finetune LLaMa-2}

    To finetune our model, we select 300 samples randomly. 100 samples for each country, and 50 samples for each source type. Afterwards, we engineered an effective prompt using \emph{Prompt Perfect} for LLaMa-2. The prompt is shown in listing \ref{listing:prompt_llama}. It should be noted that we replaced \emph{\{title\}} and \emph{\{content\}} with the actual title and content of the news article being analyzed.

    \begin{lstlisting}[caption=Prompt for LLaMa, label={listing:prompt_llama}]
As a neutral news analyst, assess the sentiment and stance of the news article
excerpt and assign a score between -1.0 (completely negative/against-{country}) 
and 1.0 (completely positive/pro-{country}) for both sentiment and stance. 
Provide a single short sentence to justify your scores, drawing on the article's
language, tone, and presentation to support your analysis.

Article Excerpt:
- Title: "{title}"
- Content: "{content}"

Output format: 
1. Sentiment: [Positive/Neutral/Negative]
    * Score: [Your Score]
    * Reason: [Your Reason] 
2. Stance: [Pro-{country}/Impartial/Against-{country}]
    * Score: [Your Score]
    * Reason: [Your Reason]
    \end{lstlisting}
    
    To create the finetuning dataset, we fed the 300 articles into ChatGPT using the prompt structure shown above. Next, we finetuned the base model from HuggingFace user \emph{TinyPixel} in the following way:

    \begin{itemize}
        \item \textbf{Sharded Base model}: A sharded base model allowed us to load chunks of the model one at a time. This, along with QLoRA, allowed us to train the model on a single GPU in google colab.
        \item \textbf{QLoRA}: We used QLoRA to quantize the model and train a 4-bit version of the LLM. Simply, this allows us to train a subset of the model's parameters instead of the entire model. This allowed us to finetune the model with only a single GPU. \cite{dettmers2023qlora}
        \item \textbf{PEFT}: Parameter Efficient Fine Tuning was used to train a subset of the model's parameters instead of the entire model. This allowed us to train the model quickly while retaining high performance.
    \end{itemize}

    After finetuning the model, 4000 samples were selected for each country. Then the sentiment and stance of those countries were predicted using LLaMa-2.

    \subsection{Hypothesis Testing}
    After getting the scores for our dataset, we performed statistical analysis on the scores of the news. In particular, we performed the following hypothesis tests:

    \begin{table}[hp]
        \centering
        \begin{tabular}{ | m{2cm} | m{3cm}| m{8cm} | } 
          \hline
          \textbf{Test} & \textbf{Parameter of Interest} & \textbf{The Question We Ask} \\ 
          \hline
            Welch Test & Mean & Does the average score from both sources have any difference? \\
            \hline
            Wilcoxon Test & Median & Is the meidan score from sources different? \\
            \hline
            Variance Test & Varaince & Does the content from both sources cover a similar range of sentiment or stance? \\
            \hline
            Pearson's Test & Correlation & Is there any linear correlation between the scores from both sources? \\
            \hline
            Spearman's Test & Monotonic Relation & Is there any monotonic relationship between the scores from both sources? \\
            \hline
        \end{tabular}

        \caption[Prompt Engineering]{Role and Goal prompt engineering and output}
        \label{table:prompt_engineering}
    \end{table}

    All tests were done with a confidence level of 99\%. Since we have a lot of data, we wanted to be sure to come to a conclusion. 

    \section{Experiment}
    
    We break our expriment into 3 case studies. Each case study covers a specific country and analyzes the news from both local and international sources.

    \subsection{Canada}
    From figure \ref{fig:canada_topic}, we can see than Canadian news covers a wide range of topics.
    
    \begin{figure}[hp]
        \centering
        \includegraphics[width=\linewidth]{../images/plots/Canada/canada_barchart_topics.png}
        \caption{Canadian news topics bar chart.}
        \label{fig:canada_topic}
    \end{figure}

    \subsubsection{Sentiment Analysis}

    We first begin by showing the distribution of the sentiment scores for each topic from both the local and international media in figure \ref{fig:canada_sentiment_score_boxplot}. 
    
    \begin{figure}[hp]
        \centering
        \includegraphics[width=\linewidth]{../images/plots/Canada/canada_boxplot_sentiment.png}
        \caption{Boxplot showing the topicwise distribution of canadian news sentiment scores for local and international media outlets.}
        \label{fig:canada_sentiment_score_boxplot}
    \end{figure}

    Some distinctions are clearly visible here. For example:

    \begin{itemize}
        \item \emph{Sports} news from both sources tends to have a positive sentiment. Both sources seem to have the same sentiment in this topic.
        \item \emph{Telecommunication} news from international media seem to cover a wide spectrum of sentiment while local media seem to convey a more negative portrayal of the matter. \emph{Public-service} has the same distribution while \emph{Crime} has the opposite.
        \item \emph{Transportation} and \emph{weather} has a noticable distinction in their score distribution for both sources.
    \end{itemize}

    We run statistical inference tests to find these distinctions mathematically. The results are shown in figure \ref{fig:canada_sentiment_score_heatmap}.

    \begin{figure}[hp]
        \centering
        \includegraphics[width=\linewidth]{../images/plots/Canada/canada_heatmap_inference_sentiment.png}
        \caption{Heatmap showing the result of topicwise inference tests done on canadian news sentiment scores.}
        \label{fig:canada_sentiment_score_heatmap}
    \end{figure}

    We can see that

    \begin{itemize}
        \item \emph{Crime}, \emph{transportation}, and \emph{weather} is statistically significant in terms of Welch and Wilcoxon test. Which means that on average the sentiment scores from both sources are different. This means both sources typically report news that are of different sentiment.
        \item \emph{National-security} is statistically significant in terms of Pearson's and Spearman's test. Which means the score of the two sources aren't independent of each other. There exists some sort of a correlation or monotonic relationship. This means that the two sources report news that are somewhat related. 
    \end{itemize}

    Let's look at a wordcloud for \emph{Crime} related news from both sources in figure \ref{fig:canada_sentiment_score_wordcloud}.

    \begin{figure}[hp]
        \centering
        \includegraphics[width=\linewidth]{../images/plots/Canada/canada_wordcloud_crime.png}
        \caption{Word cloud for canadian news about crime.}
        \label{fig:canada_sentiment_score_wordcloud}
    \end{figure}

    We can see that local and international media use the same sort of words across their negative and positive sentiment news. For example, the word police, fire, government appears in the negative sentiment word cloud for both sources. On the other hand, indigenous community, first, fire, etc. are common in the positive sentiment.

    \subsubsection{Stance Analysis}

    Now let's analyze the stance scores from figure \ref{fig:canada_stance_score_boxplot}. The most interesting observation here is that, international media, in most topics, seem to have an impartial stance, While local news outlet has a more pro-canada stance. The obvious exceptions are \emph{Health} and \emph{Food}.
    
    \begin{figure}[hp]
        \centering
        \includegraphics[width=\linewidth]{../images/plots/Canada/canada_boxplot_stance.png}
        \caption{Boxplot showing the topicwise distribution of canadian news stance scores for local and international media outlets.}
        \label{fig:canada_stance_score_boxplot}
    \end{figure}

    From the statistical test results shown in \ref{fig:canada_stance_score_heatmap}, we see a similar result as the sentiment score inference results. 

    \begin{figure}[hp]
        \centering
        \includegraphics[width=\linewidth]{../images/plots/Canada/canada_heatmap_inference_stance.png}
        \caption{Heatmap showing the result of topicwise inference tests done on canadian news stance scores.}
        \label{fig:canada_stance_score_heatmap}
    \end{figure}
    
    \begin{itemize}
        \item \emph{Crime} and \emph{Sport} relatd news seem to have different stance scores on average.
        \item While \emph{Conflict} related news isn't independent according to Pearson's and Spearman's test.
        \item Interestingly, \emph{Finance} news has a different variance for stance scores. This means, the two sources cover a different range of stances.
    \end{itemize}

    \subsubsection{Verdict}

    Although there is statistically significant distinctions in sentiment for a few topics, both sources report news that are similar in sentiment and stance. Which is why, we find canadian news organization "Global News" as a free press organization.
    
    \subsection{China}

    \subsection{Russia}

    \section{Conclusion}

    \bibliographystyle{plain}
    \bibliography{references}

    
\end{document}